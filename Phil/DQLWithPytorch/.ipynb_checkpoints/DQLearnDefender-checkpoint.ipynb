{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,  32, 8, stride=4, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.fcl = nn.Linear(128*19*8, 512)\n",
    "        self.fc2 = nn.Linear(512,6)\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=alpha)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, observation):\n",
    "        observation = T.Tensor(observation).to(self.device)\n",
    "        observation = observation.view(-1, 1, 185, 95)\n",
    "        observation = F.relu(self.conv1(observation))\n",
    "        observation = F.relu(self.conv2(observation))\n",
    "        observation = F.relu(self.conv3(observation))\n",
    "        observation = observation.view(-1, 128*19*8)\n",
    "        observation = F.relu(fcl(observation))\n",
    "        actions = self.fc2(observation)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, gamma, epsilon, alpha, max_memory_size, eps_end=0.05, replace=10000, action_space=[0,1,2,3,4,5]):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_end = eps_end\n",
    "        self.action_space = action_space\n",
    "        self.mem_size = max_memory_size\n",
    "        self.steps = 0\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = []\n",
    "        self.mem_cntr = 0\n",
    "        self.replace_target_cnt = replace\n",
    "        self.q_eval = DeepQNetwork(alpha)\n",
    "        self.q_next = DeepQNetwork(alpha)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_):\n",
    "        if self.mem_cntr < self.mem_size:\n",
    "            self.memory.append([state, action, reward, state_])\n",
    "        else:\n",
    "            self.memory[self.mem_cntr%self.mem_size] = [state, action, reward, state_]\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        rand = np.action.random()\n",
    "        actions = self.q_eval.forward(observation)\n",
    "        if rand < 1-self.epsilon:\n",
    "            action = T.argmax(actions[1]).item()\n",
    "        else:\n",
    "            action = np.random.choise(self.action_space)\n",
    "        self.step += 1\n",
    "        return action\n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        if self.replace_target_cnt is not None and self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self-q_eval-state_dict())\n",
    "            \n",
    "        if self.mem_cntr + batch_size < self.mem_size:\n",
    "            mem_start = int(np.random.choice(range(self.mem_cntr)))\n",
    "        else:\n",
    "            mem_start = int(np.random.choice(range(self.mem_cntr-batch_size-1)))\n",
    "        mini_batch = self.memory[mem_start:mem_start + batch_size]\n",
    "        memory = np.array(mini_batch)\n",
    "        \n",
    "        q_pred = self.q_eval.forward(list(memory[:, 0][:])).to(self.q_eval.device)\n",
    "        q_next = self-q_next.forward(list(memory[:, 3][:])).to(self.q_eval.device)\n",
    "        \n",
    "        max_a = T.maxarg(q_next, dim=1).to(self.q_eval.device)\n",
    "        rewards = T.Tensor(list(memory[:, 2])).to(self.q_eval.device)\n",
    "        \n",
    "        q_target = q_pred\n",
    "        q_target[:, max_a] = rewards + self.gamma*T.max(q_next[1])\n",
    "        \n",
    "        if self.steps > 500:\n",
    "            if self.epsilon -1e-4 > self.eps_end:\n",
    "                self.epsilon -= 1e-4\n",
    "            else:\n",
    "                self.epsilon = self.eslf.eps_end\n",
    "        \n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\Anaconda3\\envs\\GymPlusPytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:117: UserWarning: \n",
      "    Found GPU0 GeForce GTX 650 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done initializing memory\n",
      "staring game  1 epsilon: 1.0000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "brain = Agent(gamma=0.95, epsilon=1.0, alpha=0.03, max_memory_size=5000, replace=None)\n",
    "while brain.mem_cntr < brain.mem_size:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        if done and info['ale.lives'] == 0:\n",
    "            reward = -100\n",
    "        brain.store_transition(np.mean(observation[15:200, 30:125], axis=2), action, reward, \n",
    "                               np.mean(observation_[15:200, 30:125], axis=2))\n",
    "        observation = observation_\n",
    "print('done initializing memory')\n",
    "\n",
    "scores = []\n",
    "eps_history = []\n",
    "num_games = 50\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(num_games):\n",
    "    print('starting game ', i + 1, 'epsilon: %.4f' % brain.epsilon)\n",
    "    eps_history.append(brain.epsilon)\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    frames = [np.sum(observation[15:200, 30:125], axis=2)]\n",
    "    score = 0\n",
    "    last_action = 0\n",
    "    while not done:\n",
    "        if len(frames) == 3:\n",
    "            action = brain.choose_action(frames)\n",
    "            frames=[]\n",
    "        else:\n",
    "            action = last_action\n",
    "            \n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    frames.append(np.mean(observation_[15:200, 30:125], axis=2))\n",
    "    if done and info['ale.lives'] == 0:\n",
    "        reward = -100\n",
    "    brain.store_transition(np.mean(observation[15:200, 30:125], axis=2), action, reward, \n",
    "                           np.mean(observation_[15:200, 30:125], axis=2))\n",
    "    observation = observation_\n",
    "    brain.learn(batch_size)\n",
    "    last_action = action\n",
    "    env.render()\n",
    "scores.append(score)\n",
    "print('score: ', score)\n",
    "x = [i + 1 for i in range(num_games)]\n",
    "file_name = 'test' + str(num_games) + '.png'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
